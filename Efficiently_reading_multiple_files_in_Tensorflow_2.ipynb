{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TY6e5E04yaGq"
   },
   "source": [
    "# Efficiently reading multiple files in Tensorflow 2\n",
    "Author: [Biswajit Sahoo](https://biswajitsahoo1111.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Whether this method is efficient or not is contestable. Efficiency of a pipeline depends on many factors. How efficiently data are loaded? What is the computer architecture on which computations are being done? Is GPU available? And the list goes on. So readers might get different performance results when they run this method on their own system. The system on which we ran this notebook has 44 CPU cores. `Tensorflow` version is 2.2.0 and it is `XLA` enabled. We did not use any GPU. We achieved 20% improvement over naive method. For one personal application, involving moderate size data (3-4 GB), I achieved 10x performance improvement. So I hope that this method can be applied for other applications as well. Pleses note that for some weird reason, the speedup technique doesn't work in `Google Colab`. But it works in GPU enabled personal systems, that I have checked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TE8O8DYfyaGw"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/biswajitsahoo1111/cbm_codes_open/blob/master/notebooks/Reading_multiple_files_in_Tensorflow_2.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://www.dropbox.com/s/o4aevvuqr39kq20/Reading_multiple_files_in_Tensorflow_2.ipynb?dl=1\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E3yGhhkvyaG0"
   },
   "source": [
    "This post is a sequel to [an older post](https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/). In the previous post, we discussed ways in which we can read multiple files in `Tensorflow 2`. If our aim is only to read files without doing any transformation on data, that method might work well for most applications. But if we need to make complex transformations on data before training our deep learning algorithm, the old method might turn out to be slow. In this post, we will describe a way in which we can speedup that process. The transformations that we will consider are `spectrogram` and normalizing (converting each value to a standard normal value). We have chosen these transformtions just to illustrate the point. Readers can use any transformation (or no transformation) of their choice. More details regarding improving data performance can be found in this [tensorflow guide](https://www.tensorflow.org/guide/data_performance).\n",
    "\n",
    "As this post is a sequel, we expect readers to be familiar with the old post. We will not elaborate on points that have already been discussed. Rather, we will focus on [section 4](#speedup) which is the main topic of this post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FI06iF4gyaG3"
   },
   "source": [
    "## Outline:\n",
    "1. [Create 500 `\".csv\"` files and save it in the folder \"random_data\" in current directory.](#create_files)\n",
    "2. [Write a generator that reads data from the folder in chunks and transforms it.](#generator)\n",
    "3. [Build data pipeline and train a CNN model.](#model)\n",
    "4. [How to make the code run faster?](#speedup)\n",
    "5. [How to make predictions?](#predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0Kf-VCCyaHA"
   },
   "source": [
    "<a id = \"create_files\"></a>\n",
    "\n",
    "## 1. Create 500 `.csv` files of random data\n",
    "\n",
    "As we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. Following is the process that we will follow.\n",
    "* Each `.csv` file will have one column of data with 1024 entries.\n",
    "* Each file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in \"Fault_1\" \n",
    "category will have names as \"Fault_1_001.csv\", \"Fault_1_002.csv\", \"Fault_1_003.csv\", ..., \"Fault_1_100.csv\". Similarly for other classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-NYL4SqyaHD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "np.random.seed(1111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BSM2O-PoyaHV"
   },
   "source": [
    "First create a function that will generate random files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CtaB552PyaHY"
   },
   "outputs": [],
   "source": [
    "def create_random_csv_files(fault_classes, number_of_files_in_each_class):\n",
    "    os.mkdir(\"./random_data/\")  # Make a directory to save created files.\n",
    "    for fault_class in fault_classes:\n",
    "        for i in range(number_of_files_in_each_class):\n",
    "            data = np.random.rand(1024,)\n",
    "            file_name = \"./random_data/\" + eval(\"fault_class\") + \"_\" + \"{0:03}\".format(i+1) + \".csv\" # This creates file_name\n",
    "            np.savetxt(eval(\"file_name\"), data, delimiter = \",\", header = \"V1\", comments = \"\")\n",
    "        print(str(eval(\"number_of_files_in_each_class\")) + \" \" + eval(\"fault_class\") + \" files\"  + \" created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bh5T9WwPyaHm"
   },
   "source": [
    "Now use the function to create 100 files each for five fault types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "H7Syn-4UyaHp",
    "outputId": "19f88821-0212-4650-b8fe-5ca0d3f76c2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Fault_1 files created.\n",
      "100 Fault_2 files created.\n",
      "100 Fault_3 files created.\n",
      "100 Fault_4 files created.\n",
      "100 Fault_5 files created.\n"
     ]
    }
   ],
   "source": [
    "create_random_csv_files([\"Fault_1\", \"Fault_2\", \"Fault_3\", \"Fault_4\", \"Fault_5\"], number_of_files_in_each_class = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "mpFtOjIlyaH7",
    "outputId": "312d0a9f-45b8-48d6-c529-cdf88c96eefe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  500\n",
      "Showing first 10 files...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['./random_data/Fault_1_001.csv', './random_data/Fault_1_002.csv',\n",
       "       './random_data/Fault_1_003.csv', './random_data/Fault_1_004.csv',\n",
       "       './random_data/Fault_1_005.csv', './random_data/Fault_1_006.csv',\n",
       "       './random_data/Fault_1_007.csv', './random_data/Fault_1_008.csv',\n",
       "       './random_data/Fault_1_009.csv', './random_data/Fault_1_010.csv'],\n",
       "      dtype='<U29')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = np.sort(glob.glob(\"./random_data/*\"))\n",
    "print(\"Total number of files: \", len(files))\n",
    "print(\"Showing first 10 files...\")\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7M_ctgvxyaIK"
   },
   "source": [
    "To extract labels from file name, extract the part of the file name that corresponds to fault type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cwGdOzzSyaIN",
    "outputId": "00812a2e-2f29-4884-be0d-c0724d48e5e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./random_data/Fault_1_001.csv\n"
     ]
    }
   ],
   "source": [
    "print(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y5xv6ZZHyaIc",
    "outputId": "8433ae18-1f1f-4463-fcb4-45189a504e67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fault_1\n"
     ]
    }
   ],
   "source": [
    "print(files[0][14:21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-20C9f3dyaIq"
   },
   "source": [
    "Now that data have been created, we will go to the next step. That is, define a generator, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it. \n",
    "\n",
    "<a id = \"generator\"></a>\n",
    "\n",
    "## 2. Write a generator that reads data in chunks and preprocesses it\n",
    "\n",
    "These are the few things that we want our generator to have.\n",
    "\n",
    " 1. It should run indefinitely, i.e., it is an infinite loop.\n",
    " 2. Inside generator loop, read individual files using `pandas`.\n",
    " 3. Do transformations on data if required.\n",
    " 4. Yield the data.\n",
    " \n",
    "As we will be solving a classification problem, we have to assign labels to each raw data. We will use following labels for convenience.\n",
    "\n",
    "|Class| Label|\n",
    "|-----|------|\n",
    "|Fault_1| 0|\n",
    "|Fault_2| 1|\n",
    "|Fault_3| 2|\n",
    "|Fault_4| 3|\n",
    "|Fault_5| 4|\n",
    "\n",
    "The generator will `yield` both data and labels. The generator takes a list of file names as first argument. The second argument is `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZOOZnjH2qLU"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8J4XAOOF0VPh"
   },
   "outputs": [],
   "source": [
    "def tf_data_generator(file_list, batch_size = 20):\n",
    "    i = 0\n",
    "    while True:    # This loop makes the generator an infinite loop\n",
    "        if i*batch_size >= len(file_list):  \n",
    "            i = 0\n",
    "            np.random.shuffle(file_list)\n",
    "        else:\n",
    "            file_chunk = file_list[i*batch_size:(i+1)*batch_size] \n",
    "            data = []\n",
    "            labels = []\n",
    "            label_classes = tf.constant([\"Fault_1\", \"Fault_2\", \"Fault_3\", \"Fault_4\", \"Fault_5\"]) \n",
    "            for file in file_chunk:\n",
    "                temp = pd.read_csv(open(file,'r')).astype(np.float32)    # Read data\n",
    "                #########################################################################################################\n",
    "                # Apply transformations. Comment this portion if you don't have to do any.\n",
    "                # Try to use Tensorflow transformations as much as possible. First compute a spectrogram.\n",
    "                temp = tf.math.abs(tf.signal.stft(tf.reshape(temp.values, shape = (1024,)),frame_length = 64, frame_step = 32, fft_length = 64))\n",
    "                # After STFT transformation with given parameters, shape = (31,33)\n",
    "                temp = tf.image.per_image_standardization(tf.reshape(temp, shape = (-1,31,33,1))) # Image Normalization\n",
    "                ##########################################################################################################\n",
    "                # temp = tf.reshape(temp, (32,32,1)) # Uncomment this line if you have not done any transformation.\n",
    "                data.append(temp)\n",
    "                pattern = tf.constant(eval(\"file[14:21]\"))  \n",
    "                for j in range(len(label_classes)):\n",
    "                    if re.match(pattern.numpy(), label_classes[j].numpy()): \n",
    "                        labels.append(j)\n",
    "            data = np.asarray(data).reshape(-1,31,33,1) \n",
    "            labels = np.asarray(labels)\n",
    "            yield data, labels\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hc_oIsnzJulQ"
   },
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "dataset = tf.data.Dataset.from_generator(tf_data_generator,args= [files, batch_size],output_types = (tf.float32, tf.float32),\n",
    "                                                output_shapes = ((None,31,33,1),(None,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "g7U-GXVGB3__",
    "outputId": "fec81efd-5813-4aed-e721-a63e55a54b3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 31, 33, 1)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\n",
      "(15, 31, 33, 1)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\n",
      "(15, 31, 33, 1)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\n",
      "(15, 31, 33, 1)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\n",
      "(15, 31, 33, 1)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\n",
      "(15, 31, 33, 1)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(15,), dtype=float32)\n",
      "(15, 31, 33, 1)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.], shape=(15,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for data, labels in dataset.take(7):\n",
    "  print(data.shape)\n",
    "  print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jn7cC0TCyaLC"
   },
   "source": [
    "The generator works fine. Now, we will train a full CNN model using the generator. As is done in every model, we will first shuffle data files. Split the files into train, validation, and test set. Using the `tf_data_generator` create three tensorflow datasets corresponding to train, validation, and test data respectively. Finally, we will create a simple CNN model. Train it using train dataset, see its performance on validation dataset, and obtain prediction using test dataset. Keep in mind that our aim is not to improve performance of the model. As the data are random, don't expect to see good performance. The aim is only to create a pipeline. \n",
    "\n",
    "<a id = \"model\"></a>\n",
    "\n",
    "## 3. Building data pipeline and training a CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NN87f4KryaLF"
   },
   "source": [
    "Before building the data pipeline, we will first move files corresponding to each fault class into different folders. This will make it convenient to split data into training, validation, and test set, keeping the balanced nature of the dataset intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKlaBmTjyaLH"
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DrNsud-2yaLT"
   },
   "source": [
    "Create five different folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2e-ktj2yaLV"
   },
   "outputs": [],
   "source": [
    "fault_folders = [\"Fault_1\", \"Fault_2\", \"Fault_3\", \"Fault_4\", \"Fault_5\"]\n",
    "for folder_name in fault_folders:\n",
    "    os.mkdir(os.path.join(\"./random_data\", folder_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MxXACTgUyaLg"
   },
   "source": [
    "Move files into those folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u0UNlgM-yaLi"
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    pattern = \"^\" + eval(\"file[14:21]\")\n",
    "    for j in range(len(fault_folders)):\n",
    "        if re.match(pattern, fault_folders[j]):\n",
    "            dest = os.path.join(\"./random_data/\",eval(\"fault_folders[j]\"))\n",
    "            shutil.move(file, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "ZKXOT3nvyaLt",
    "outputId": "84ea212e-bb7a-4c72-9c63-151884412b01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./random_data/Fault_1',\n",
       " './random_data/Fault_2',\n",
       " './random_data/Fault_3',\n",
       " './random_data/Fault_4',\n",
       " './random_data/Fault_5']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./random_data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "WYJha7PvyaL-",
    "outputId": "70632964-1d01-43c8-a187-cc408a1b56db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['./random_data/Fault_1/Fault_1_001.csv',\n",
       "       './random_data/Fault_1/Fault_1_002.csv',\n",
       "       './random_data/Fault_1/Fault_1_003.csv',\n",
       "       './random_data/Fault_1/Fault_1_004.csv',\n",
       "       './random_data/Fault_1/Fault_1_005.csv',\n",
       "       './random_data/Fault_1/Fault_1_006.csv',\n",
       "       './random_data/Fault_1/Fault_1_007.csv',\n",
       "       './random_data/Fault_1/Fault_1_008.csv',\n",
       "       './random_data/Fault_1/Fault_1_009.csv',\n",
       "       './random_data/Fault_1/Fault_1_010.csv'], dtype='<U37')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(glob.glob(\"./random_data/Fault_1/*\"))[:10] # Showing first 10 files of Fault_1 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "xb7bBLhUyaMK",
    "outputId": "2d04c95a-1cc2-4c6d-825d-f716382f4769"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['./random_data/Fault_3/Fault_3_001.csv',\n",
       "       './random_data/Fault_3/Fault_3_002.csv',\n",
       "       './random_data/Fault_3/Fault_3_003.csv',\n",
       "       './random_data/Fault_3/Fault_3_004.csv',\n",
       "       './random_data/Fault_3/Fault_3_005.csv',\n",
       "       './random_data/Fault_3/Fault_3_006.csv',\n",
       "       './random_data/Fault_3/Fault_3_007.csv',\n",
       "       './random_data/Fault_3/Fault_3_008.csv',\n",
       "       './random_data/Fault_3/Fault_3_009.csv',\n",
       "       './random_data/Fault_3/Fault_3_010.csv'], dtype='<U37')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(glob.glob(\"./random_data/Fault_3/*\"))[:10] # Showing first 10 files of Falut_3 folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z5Bl2RTgyaMW"
   },
   "source": [
    "Prepare that data for training set, validation set, and test_set. For each fault type, we will keep 70 files for training, 10 files for validation and 20 files for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhRyg9gLyaMY"
   },
   "outputs": [],
   "source": [
    "fault_1_files = glob.glob(\"./random_data/Fault_1/*\")\n",
    "fault_2_files = glob.glob(\"./random_data/Fault_2/*\")\n",
    "fault_3_files = glob.glob(\"./random_data/Fault_3/*\")\n",
    "fault_4_files = glob.glob(\"./random_data/Fault_4/*\")\n",
    "fault_5_files = glob.glob(\"./random_data/Fault_5/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aa3i8W6WyaMk"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9DonCNfkyaMv"
   },
   "outputs": [],
   "source": [
    "fault_1_train, fault_1_test = train_test_split(fault_1_files, test_size = 20, random_state = 5)\n",
    "fault_2_train, fault_2_test = train_test_split(fault_2_files, test_size = 20, random_state = 54)\n",
    "fault_3_train, fault_3_test = train_test_split(fault_3_files, test_size = 20, random_state = 543)\n",
    "fault_4_train, fault_4_test = train_test_split(fault_4_files, test_size = 20, random_state = 5432)\n",
    "fault_5_train, fault_5_test = train_test_split(fault_5_files, test_size = 20, random_state = 54321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2JI0qQ8PyaM8"
   },
   "outputs": [],
   "source": [
    "fault_1_train, fault_1_val = train_test_split(fault_1_train, test_size = 10, random_state = 1)\n",
    "fault_2_train, fault_2_val = train_test_split(fault_2_train, test_size = 10, random_state = 12)\n",
    "fault_3_train, fault_3_val = train_test_split(fault_3_train, test_size = 10, random_state = 123)\n",
    "fault_4_train, fault_4_val = train_test_split(fault_4_train, test_size = 10, random_state = 1234)\n",
    "fault_5_train, fault_5_val = train_test_split(fault_5_train, test_size = 10, random_state = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hM5IrxLTyaNI"
   },
   "outputs": [],
   "source": [
    "train_file_names = fault_1_train + fault_2_train + fault_3_train + fault_4_train + fault_5_train\n",
    "validation_file_names = fault_1_val + fault_2_val + fault_3_val + fault_4_val + fault_5_val\n",
    "test_file_names = fault_1_test + fault_2_test + fault_3_test + fault_4_test + fault_5_test\n",
    "\n",
    "# Shuffle files\n",
    "np.random.shuffle(train_file_names)\n",
    "np.random.shuffle(validation_file_names)\n",
    "np.random.shuffle(test_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "mR6cqYjbyaNT",
    "outputId": "6778761b-94ac-48a4-ad0e-b83e963ad764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train_files: 350\n",
      "Number of validation_files: 50\n",
      "Number of test_files: 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train_files:\" ,len(train_file_names))\n",
    "print(\"Number of validation_files:\" ,len(validation_file_names))\n",
    "print(\"Number of test_files:\" ,len(test_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tM5jK2nYyaNg"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [train_file_names, batch_size], \n",
    "                                              output_shapes = ((None,31,33,1),(None,)),\n",
    "                                              output_types = (tf.float32, tf.float32))\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [validation_file_names, batch_size],\n",
    "                                                   output_shapes = ((None,31,33,1),(None,)),\n",
    "                                                   output_types = (tf.float32, tf.float32))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [test_file_names, batch_size],\n",
    "                                             output_shapes = ((None,31,33,1),(None,)),\n",
    "                                             output_types = (tf.float32, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKMYq9_zyaNs"
   },
   "source": [
    "Now create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ZvvXcMgyaNu"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "UCd-X_J-yaN5",
    "outputId": "a8ade574-b523-4776-c3f7-a619e1edc5df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 29, 31, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 15, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 13, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                18448     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 23,333\n",
      "Trainable params: 23,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(16, 3, activation = \"relu\", input_shape = (31,33,1)),\n",
    "    layers.MaxPool2D(2),\n",
    "    layers.Conv2D(32, 3, activation = \"relu\"),\n",
    "    layers.MaxPool2D(2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(16, activation = \"relu\"),\n",
    "    layers.Dense(5, activation = \"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JsCpI4RxyaON"
   },
   "source": [
    "Compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mdew95styaOR"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ev1-HRdyyaOd"
   },
   "source": [
    "Before we fit the model, we have to do one important calculation. Remember that our generators are infinite loops. So if no stopping criteria is given, it will run indefinitely. But we want our model to run for, say, 10 epochs. So our generator should loop over the data files just 10 times and no more. This is achieved by setting the arguments `steps_per_epoch` and `validation_steps` to desired numbers in `model.fit()`. Similarly while evaluating model, we need to set the argument `steps` to a desired number in `model.evaluate()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d3ZMZIe8yaOf"
   },
   "source": [
    "There are 350 files in training set. Batch_size is 10. So if the generator runs 35 times, it will correspond to one epoch. Therefor, we should set `steps_per_epoch` to 35. Similarly, `validation_steps = 5` and in `model.evaluate()`, `steps = 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "cQJKmopUyaOh",
    "outputId": "fc84a119-0cf0-42d9-f772-d45a81d6690b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_per_epoch =  11\n",
      "validation_steps =  2\n",
      "steps =  4\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = np.int(np.ceil(len(train_file_names)/batch_size))\n",
    "validation_steps = np.int(np.ceil(len(validation_file_names)/batch_size))\n",
    "steps = np.int(np.ceil(len(test_file_names)/batch_size))\n",
    "print(\"steps_per_epoch = \", steps_per_epoch)\n",
    "print(\"validation_steps = \", validation_steps)\n",
    "print(\"steps = \", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "8m1vdtr1yaOr",
    "outputId": "7c7bb555-ae7e-43b6-f245-19c06f0d88f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11/11 [==============================] - 64s 6s/step - loss: 1.6222 - accuracy: 0.1486 - val_loss: 1.6067 - val_accuracy: 0.1800\n",
      "Epoch 2/5\n",
      "11/11 [==============================] - 65s 6s/step - loss: 1.6088 - accuracy: 0.2200 - val_loss: 1.6078 - val_accuracy: 0.2000\n",
      "Epoch 3/5\n",
      "11/11 [==============================] - 65s 6s/step - loss: 1.6090 - accuracy: 0.2029 - val_loss: 1.6088 - val_accuracy: 0.2000\n",
      "Epoch 4/5\n",
      "11/11 [==============================] - 65s 6s/step - loss: 1.6003 - accuracy: 0.2886 - val_loss: 1.6075 - val_accuracy: 0.1800\n",
      "Epoch 5/5\n",
      "11/11 [==============================] - 66s 6s/step - loss: 1.5956 - accuracy: 0.3229 - val_loss: 1.6073 - val_accuracy: 0.2200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc3b818dc50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, validation_data = validation_dataset, steps_per_epoch = steps_per_epoch,\n",
    "         validation_steps = validation_steps, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nmls7f-ayaO4",
    "outputId": "824b53d9-4031-4a97-c1bc-ce48599b5762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 13s 3s/step - loss: 1.6098 - accuracy: 0.2000\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset, steps = steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "RoOR861ZyaPR",
    "outputId": "55070544-f763-482d-e6e4-8550cc42c9e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  1.6098381280899048\n",
      "Test accuracy: 0.20000000298023224\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: \", test_loss)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hS7p4hzayaPb"
   },
   "source": [
    "As expected, model performs terribly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfTiaOEZyaPd"
   },
   "source": [
    "<a id = \"speedup\"></a>\n",
    "\n",
    "## How to make the code run faster?\n",
    "\n",
    "If no transformations are used, just using `prefetch` might improve performance. In deep learing usually GPUs are used for training. But all the data processing is done in CPU. In the naive approach, we will first process data in CPU, then send the processed data to GPU and after training finishes, we will prepare another batch of data. This approch is not efficient because GPU has to wait for data to get prepared. But using `prefetch`, we prepare and keep ready batches of data while training continues. In this way, waiting time of GPU is minimized.\n",
    "\n",
    "When data transformations are used, out aim should always be to use parallel processing capabilites of `tensorflow`. We can achieve this using `map` function. Inside the `map` function, all transformations are defined. Then we can `prefetch` batches to further improve performance. The whole pipeline is as follows.\n",
    "\n",
    "```\n",
    "1. def transformation_function(...):\n",
    "    # Define all transormations (STFT, Normalization, etc.)\n",
    "    \n",
    "2. def generator(...):\n",
    "    \n",
    "       # Read data\n",
    "    \n",
    "       # Call transformation_function using tf.data.Dataset.map so that it can parallelize operations.\n",
    "    \n",
    "       # Finally yield the processed data\n",
    "\n",
    "3. Create tf.data.Dataset s.\n",
    "\n",
    "4. Prefecth datasets.\n",
    "\n",
    "5. Create model and train it.\n",
    "```\n",
    "We will use one extra library `tensorflow_datasets` that will allow us to switch from `tf.dataset` to `numpy`. If `tensorflow_datasets` is not installed in your system, use `pip install tensorflow-datasets` to install it and then run following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PlU5bos-mW-k"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQE_YW-vkc4i"
   },
   "outputs": [],
   "source": [
    "def data_transformation_func(data):\n",
    "  transformed_data = tf.math.abs(tf.signal.stft(data,frame_length = 64, frame_step = 32, fft_length = 64))\n",
    "  transformed_data = tf.image.per_image_standardization(tf.reshape(transformed_data, shape = (-1,31,33,1))) # Normalization\n",
    "  return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hBPu0OB7lJi2"
   },
   "outputs": [],
   "source": [
    "def tf_data_generator_new(file_list, batch_size = 4):\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i*batch_size >= len(file_list):  \n",
    "            i = 0\n",
    "            np.random.shuffle(file_list)\n",
    "        else:\n",
    "            file_chunk = file_list[i*batch_size:(i+1)*batch_size]\n",
    "            data = []\n",
    "            labels = []\n",
    "            label_classes = tf.constant([\"Fault_1\", \"Fault_2\", \"Fault_3\", \"Fault_4\", \"Fault_5\"]) \n",
    "            for file in file_chunk:\n",
    "                temp = pd.read_csv(open(file,'r')).astype(np.float32)    # Read data\n",
    "                data.append(tf.reshape(temp.values, shape = (1,1024)))\n",
    "                pattern = tf.constant(eval(\"file[22:29]\"))\n",
    "                for j in range(len(label_classes)):\n",
    "                    if re.match(pattern.numpy(), label_classes[j].numpy()): \n",
    "                        labels.append(j)\n",
    "                    \n",
    "            data = np.asarray(data)\n",
    "            labels = np.asarray(labels)\n",
    "            first_dim = data.shape[0]\n",
    "            # Create tensorflow dataset so that we can use `map` function that can do parallel computation.\n",
    "            data_ds = tf.data.Dataset.from_tensor_slices(data)\n",
    "            data_ds = data_ds.batch(batch_size = first_dim).map(data_transformation_func,\n",
    "                                                                num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "            # Convert the dataset to a generator and subsequently to numpy array\n",
    "            data_ds = tfds.as_numpy(data_ds)   # This is where tensorflow-datasets library is used.\n",
    "            data = np.asarray([data for data in data_ds]).reshape(first_dim,31,33,1)\n",
    "            \n",
    "            yield data, labels\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "WCFMSGBhm70G",
    "outputId": "abc786b6-061d-434e-b115-e4d522a5ecc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./random_data/Fault_3/Fault_3_045.csv',\n",
       " './random_data/Fault_1/Fault_1_032.csv',\n",
       " './random_data/Fault_1/Fault_1_025.csv',\n",
       " './random_data/Fault_2/Fault_2_013.csv',\n",
       " './random_data/Fault_3/Fault_3_053.csv',\n",
       " './random_data/Fault_1/Fault_1_087.csv',\n",
       " './random_data/Fault_5/Fault_5_053.csv',\n",
       " './random_data/Fault_4/Fault_4_019.csv',\n",
       " './random_data/Fault_3/Fault_3_034.csv',\n",
       " './random_data/Fault_2/Fault_2_044.csv']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vM7l4clHqHMB",
    "outputId": "8f3e2ae1-344e-4484-a06c-4434cc55d439"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fault_3'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_names[0][22:29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANVWGZ1Bmouv"
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "dataset_check = tf.data.Dataset.from_generator(tf_data_generator_new,args= [train_file_names, batch_size],output_types = (tf.float32, tf.float32),\n",
    "                                                output_shapes = ((None,31,33,1),(None,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "q8g9XmxcrPL8",
    "outputId": "65f9b236-bf98-44d6-ee36-9efc3b3c38ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 31, 33, 1)\n",
      "tf.Tensor([2. 0. 0. 1. 2. 0. 4. 3. 2. 1. 1. 0. 3. 3. 2. 3. 1. 4. 2. 4.], shape=(20,), dtype=float32)\n",
      "(20, 31, 33, 1)\n",
      "tf.Tensor([3. 1. 1. 3. 4. 4. 2. 3. 4. 3. 3. 0. 1. 2. 0. 3. 2. 2. 2. 4.], shape=(20,), dtype=float32)\n",
      "(20, 31, 33, 1)\n",
      "tf.Tensor([2. 3. 0. 2. 2. 4. 3. 0. 4. 1. 0. 0. 2. 0. 0. 1. 0. 3. 2. 1.], shape=(20,), dtype=float32)\n",
      "(20, 31, 33, 1)\n",
      "tf.Tensor([4. 2. 2. 2. 0. 3. 4. 2. 0. 1. 2. 2. 3. 4. 0. 4. 2. 0. 4. 4.], shape=(20,), dtype=float32)\n",
      "(20, 31, 33, 1)\n",
      "tf.Tensor([1. 0. 4. 4. 0. 1. 0. 4. 0. 2. 1. 4. 3. 2. 1. 4. 4. 2. 4. 3.], shape=(20,), dtype=float32)\n",
      "(20, 31, 33, 1)\n",
      "tf.Tensor([2. 2. 0. 1. 3. 2. 2. 2. 1. 3. 3. 4. 0. 1. 4. 1. 3. 2. 1. 3.], shape=(20,), dtype=float32)\n",
      "(20, 31, 33, 1)\n",
      "tf.Tensor([2. 1. 2. 2. 4. 4. 1. 0. 2. 2. 1. 2. 3. 0. 0. 2. 2. 0. 3. 3.], shape=(20,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for data, labels in dataset_check.take(7):\n",
    "  print(data.shape)\n",
    "  print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1SFsrjuaruGw"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset_new = tf.data.Dataset.from_generator(tf_data_generator_new, args = [train_file_names, batch_size], \n",
    "                                                  output_shapes = ((None,31,33,1),(None,)),\n",
    "                                                  output_types = (tf.float32, tf.float32))\n",
    "\n",
    "validation_dataset_new = tf.data.Dataset.from_generator(tf_data_generator_new, args = [validation_file_names, batch_size],\n",
    "                                                       output_shapes = ((None,31,33,1),(None,)),\n",
    "                                                       output_types = (tf.float32, tf.float32))\n",
    "\n",
    "test_dataset_new = tf.data.Dataset.from_generator(tf_data_generator_new, args = [test_file_names, batch_size],\n",
    "                                                 output_shapes = ((None,31,33,1),(None,)),\n",
    "                                                 output_types = (tf.float32, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefetch datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fIdX8wZAsf5M"
   },
   "outputs": [],
   "source": [
    "train_dataset_new = train_dataset_new.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
    "validation_dataset_new = validation_dataset_new.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddQBQaOesu7w"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "V4XkfAa2s5yf",
    "outputId": "e1bef674-6530-4581-ce37-80839bf86252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11/11 [==============================] - 45s 4s/step - loss: 1.5939 - accuracy: 0.2714 - val_loss: 1.6075 - val_accuracy: 0.2000\n",
      "Epoch 2/5\n",
      "11/11 [==============================] - 46s 4s/step - loss: 1.5890 - accuracy: 0.2886 - val_loss: 1.6082 - val_accuracy: 0.2600\n",
      "Epoch 3/5\n",
      "11/11 [==============================] - 44s 4s/step - loss: 1.5771 - accuracy: 0.3257 - val_loss: 1.6066 - val_accuracy: 0.2000\n",
      "Epoch 4/5\n",
      "11/11 [==============================] - 44s 4s/step - loss: 1.5710 - accuracy: 0.4400 - val_loss: 1.6057 - val_accuracy: 0.2200\n",
      "Epoch 5/5\n",
      "11/11 [==============================] - 45s 4s/step - loss: 1.5564 - accuracy: 0.3771 - val_loss: 1.6074 - val_accuracy: 0.1600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc398126090>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset_new, validation_data = validation_dataset_new, steps_per_epoch = steps_per_epoch,\n",
    "         validation_steps = validation_steps, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 2s/step - loss: 1.6097 - accuracy: 0.1700\n"
     ]
    }
   ],
   "source": [
    "test_loss_new, test_acc_new = model.evaluate(test_dataset_new, steps = steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZaTQsHkyaPf"
   },
   "source": [
    "<a id = \"predictions\"></a>\n",
    "\n",
    "## How to make predictions?\n",
    "\n",
    "In the generator used for prediction, we can also use `map` function to parallelize data preprocessing. But in practice, inference is much faster. So we can make fast predictions using naive method also. We show the naive implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jaQWzivfyaPj"
   },
   "outputs": [],
   "source": [
    "def create_prediction_set(num_files = 20):\n",
    "    os.mkdir(\"./random_data/prediction_set\")\n",
    "    for i in range(num_files):\n",
    "        data = np.random.randn(1024,)\n",
    "        file_name = \"./random_data/prediction_set/\"  + \"file_\" + \"{0:03}\".format(i+1) + \".csv\" # This creates file_name\n",
    "        np.savetxt(eval(\"file_name\"), data, delimiter = \",\", header = \"V1\", comments = \"\")\n",
    "    print(str(eval(\"num_files\")) + \" \"+ \" files created in prediction set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JFuI_1XkyaPz"
   },
   "source": [
    "Create some files for prediction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IjD7BEYcyaP1",
    "outputId": "654f67ae-ff63-4bcc-c4fa-b59241d7611d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55  files created in prediction set.\n"
     ]
    }
   ],
   "source": [
    "create_prediction_set(num_files = 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-vRJNI0yaQB",
    "outputId": "ad2da5b2-9c87-4c36-8139-6387603a2395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  55\n",
      "Showing first 10 files...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./random_data/prediction_set/file_001.csv',\n",
       " './random_data/prediction_set/file_002.csv',\n",
       " './random_data/prediction_set/file_003.csv',\n",
       " './random_data/prediction_set/file_004.csv',\n",
       " './random_data/prediction_set/file_005.csv',\n",
       " './random_data/prediction_set/file_006.csv',\n",
       " './random_data/prediction_set/file_007.csv',\n",
       " './random_data/prediction_set/file_008.csv',\n",
       " './random_data/prediction_set/file_009.csv',\n",
       " './random_data/prediction_set/file_010.csv']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_files = glob.glob(\"./random_data/prediction_set/*\")\n",
    "print(\"Total number of files: \", len(prediction_files))\n",
    "print(\"Showing first 10 files...\")\n",
    "prediction_files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6mwvoo4yaQW"
   },
   "source": [
    "Now, we will create a generator to read these files in chunks. This generator will be slightly different from our previous generator. Firstly, we don't want the generator to run indefinitely. Secondly, we don't have any labels. So this generator should only `yield` data. This is how we achieve that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FPd0cbg_yaQY"
   },
   "outputs": [],
   "source": [
    "def generator_for_prediction(file_list, batch_size = 20):\n",
    "    i = 0\n",
    "    while i <= (len(file_list)/batch_size):\n",
    "        if i == np.floor(len(file_list)/batch_size):\n",
    "            file_chunk = file_list[i*batch_size:len(file_list)]\n",
    "            if len(file_chunk)==0:\n",
    "                break\n",
    "        else:\n",
    "            file_chunk = file_list[i*batch_size:(i+1)*batch_size] \n",
    "        data = []\n",
    "        for file in file_chunk:\n",
    "            temp = pd.read_csv(open(file,'r')).astype(np.float32)\n",
    "            temp = tf.math.abs(tf.signal.stft(tf.reshape(temp.values, shape = (1024,)),frame_length = 64, frame_step = 32, fft_length = 64))\n",
    "            # After STFT transformation with given parameters, shape = (31,33)\n",
    "            temp = tf.image.per_image_standardization(tf.reshape(temp, shape = (-1,31,33,1))) # Image Normalization\n",
    "            data.append(temp) \n",
    "        data = np.asarray(data).reshape(-1,31,33,1)\n",
    "        yield data\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y-N3kFQAyaQj"
   },
   "source": [
    "Check whether the generator works or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWi8uiZoyaQl",
    "outputId": "b0c328c4-e88c-4e10-b79d-025066c76627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 31, 33, 1)\n",
      "(10, 31, 33, 1)\n",
      "(10, 31, 33, 1)\n",
      "(10, 31, 33, 1)\n",
      "(10, 31, 33, 1)\n",
      "(5, 31, 33, 1)\n"
     ]
    }
   ],
   "source": [
    "pred_gen = generator_for_prediction(prediction_files,  batch_size = 10)\n",
    "for data in pred_gen:\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PTccD4nxyaQt"
   },
   "source": [
    "Create a `tensorflow dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jL0Zhtn8yaQv"
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "prediction_dataset = tf.data.Dataset.from_generator(generator_for_prediction,args=[prediction_files, batch_size],\n",
    "                                                 output_shapes=(None,31,33,1), output_types=(tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7zREOpahyaQ4"
   },
   "outputs": [],
   "source": [
    "steps = np.int(np.ceil(len(prediction_files)/batch_size))\n",
    "predictions = model.predict(prediction_dataset,steps = steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfC-LPhOyaRK",
    "outputId": "9ce35deb-4ffc-4351-be62-0bc5c85d5cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prediction array:  (55, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.13783312, 0.06810743, 0.18828638, 0.4399181 , 0.16585506],\n",
       "       [0.2011155 , 0.0909321 , 0.12722781, 0.34147328, 0.23925126],\n",
       "       [0.184051  , 0.11195082, 0.15630874, 0.41264012, 0.13504937],\n",
       "       [0.17021744, 0.15275575, 0.17176864, 0.36582083, 0.13943738],\n",
       "       [0.22107455, 0.13893652, 0.16182247, 0.22719847, 0.25096804],\n",
       "       [0.16544239, 0.09297101, 0.19448881, 0.37793893, 0.16915883],\n",
       "       [0.20981115, 0.09095117, 0.1454936 , 0.37553373, 0.17821036],\n",
       "       [0.18948458, 0.08287238, 0.16043249, 0.31469837, 0.25251222],\n",
       "       [0.14806318, 0.08988151, 0.18063019, 0.43348154, 0.14794365],\n",
       "       [0.19300967, 0.17423573, 0.1853214 , 0.29504803, 0.15238515],\n",
       "       [0.14796554, 0.10064519, 0.17332935, 0.46094754, 0.11711246],\n",
       "       [0.1620164 , 0.10878453, 0.19735815, 0.28250632, 0.2493346 ],\n",
       "       [0.17244144, 0.13593125, 0.18931074, 0.3498449 , 0.1524716 ],\n",
       "       [0.16827711, 0.08276799, 0.16664039, 0.38747287, 0.19484173],\n",
       "       [0.16345006, 0.1138956 , 0.17773166, 0.39695117, 0.14797151],\n",
       "       [0.17923051, 0.12203053, 0.20120224, 0.23441198, 0.26312473],\n",
       "       [0.1487248 , 0.09016878, 0.17162901, 0.43704256, 0.15243487],\n",
       "       [0.16879848, 0.05954535, 0.14414911, 0.45952848, 0.16797861],\n",
       "       [0.14453672, 0.11703113, 0.19364771, 0.4488474 , 0.09593706],\n",
       "       [0.20345339, 0.1580022 , 0.17898531, 0.22838299, 0.23117617],\n",
       "       [0.16221416, 0.05681619, 0.14693654, 0.39674726, 0.23728591],\n",
       "       [0.200225  , 0.16417584, 0.18793206, 0.26401222, 0.18365487],\n",
       "       [0.21399722, 0.13131607, 0.17154819, 0.21897295, 0.26416558],\n",
       "       [0.1392046 , 0.05873166, 0.1671688 , 0.45915708, 0.17573787],\n",
       "       [0.16581263, 0.08813614, 0.18449506, 0.3109786 , 0.25057748],\n",
       "       [0.15438257, 0.11544537, 0.19926623, 0.37426034, 0.15664549],\n",
       "       [0.15011945, 0.08316098, 0.1305757 , 0.5280127 , 0.10813113],\n",
       "       [0.18201515, 0.12078299, 0.17261833, 0.29602036, 0.22856328],\n",
       "       [0.17936407, 0.09680162, 0.17545709, 0.2755434 , 0.27283388],\n",
       "       [0.19463587, 0.11394399, 0.17677711, 0.3607436 , 0.15389942],\n",
       "       [0.1834404 , 0.07998151, 0.16563387, 0.38610289, 0.18484135],\n",
       "       [0.20745523, 0.14513774, 0.18552025, 0.2872524 , 0.1746344 ],\n",
       "       [0.18435965, 0.15455365, 0.19811183, 0.28113118, 0.18184367],\n",
       "       [0.19053918, 0.13114992, 0.18859585, 0.28579548, 0.20391957],\n",
       "       [0.1874934 , 0.13049673, 0.15486516, 0.4116317 , 0.11551296],\n",
       "       [0.16247028, 0.09362978, 0.15978761, 0.41194388, 0.17216852],\n",
       "       [0.16956635, 0.06130224, 0.13529454, 0.41197857, 0.2218583 ],\n",
       "       [0.17781247, 0.14515027, 0.17571223, 0.34297863, 0.15834646],\n",
       "       [0.18537633, 0.14020114, 0.17088792, 0.30255666, 0.20097792],\n",
       "       [0.21270326, 0.13349937, 0.1523133 , 0.263564  , 0.23792005],\n",
       "       [0.19857787, 0.07489564, 0.1436915 , 0.29431146, 0.2885235 ],\n",
       "       [0.18926036, 0.11828965, 0.17655246, 0.24165332, 0.2742442 ],\n",
       "       [0.18234053, 0.082383  , 0.16731356, 0.30818883, 0.25977406],\n",
       "       [0.17025198, 0.08521349, 0.16441138, 0.41964525, 0.16047782],\n",
       "       [0.17940679, 0.09788707, 0.15743247, 0.35294068, 0.21233296],\n",
       "       [0.11456501, 0.05288012, 0.16385278, 0.5418783 , 0.12682377],\n",
       "       [0.15863904, 0.06855461, 0.16147587, 0.40622538, 0.2051051 ],\n",
       "       [0.19545631, 0.08327787, 0.13592716, 0.38202846, 0.20331012],\n",
       "       [0.17398484, 0.14876288, 0.18257992, 0.33674046, 0.15793191],\n",
       "       [0.21319063, 0.08506136, 0.15001011, 0.37536374, 0.17637412],\n",
       "       [0.20356631, 0.21442604, 0.20090103, 0.22577564, 0.15533103],\n",
       "       [0.18141419, 0.11649938, 0.18554828, 0.23423648, 0.2823017 ],\n",
       "       [0.15753253, 0.10006633, 0.18498763, 0.36755162, 0.18986186],\n",
       "       [0.18776654, 0.11064088, 0.20178466, 0.2612361 , 0.23857178],\n",
       "       [0.20099026, 0.14279291, 0.15887792, 0.2843657 , 0.21297309]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape of prediction array: \", predictions.shape)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMpRtA4jyaRW"
   },
   "source": [
    "Outputs of prediction are 5 dimensional vector. This is so because we have used 5 neurons in the output layer and our activation function is softmax. The 5 dimensional output vector for an input add to 1. So it can be interpreted as probability. Thus we should classify the input to a class, for which prediction probability is maximum. To get the class corresponding to maximum probability, we can use `np.argmax()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcTrbTrvyaRX",
    "outputId": "fd13ac75-f9ad-4b3f-8a4b-7ca9035c25ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 4, 3, 3,\n",
       "       4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WyaoMYlpyaRp"
   },
   "source": [
    "As a final comment, read the **note** at the beginning of this post."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "tZaTQsHkyaPf"
   ],
   "name": "Efficiently_reading_multiple_files_in_Tensorflow_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf_cpu_22",
   "language": "python",
   "name": "tf_cpu_22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
