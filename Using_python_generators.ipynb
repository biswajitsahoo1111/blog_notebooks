{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python Generators\n",
    "Author: [Biswajit Sahoo](https://biswajitsahoo1111.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/biswajitsahoo1111/blog_notebooks/blob/master/Using_python_generators.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/biswajitsahoo1111/blog_notebooks/blob/master/Using_python_generators.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://www.dropbox.com/s/ax24jc2rdmg4dlo/Using_python_generators.ipynb?dl=1\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we will discuss about generators in python. In this age of big data it is not unlikely to encounter a large dataset that can't be loaded into RAM. In such scenarios, it is natural to extract workable chunks of data and work on it. Generators help us do just that. Generators are almost like functions but with a vital difference. While functions produce all their outputs at once, generators produce their outputs one by one and that too when asked. Much has been written about generators. So our aim is not to restate those again. We would rather give two toy examples showing how generators work. Hopefully, these examples will be useful for beginners.\n",
    "\n",
    "While functions use keyword return to produce outputs, generators use yield. Use of yield in a function automatically makes that function a generator. We can write generators that work for few iterations or indefinitely (It's an infinite loop). Deep learning frameworks like Keras expect the generators to work indefinitely. So we will also write generators that work indefinitely.\n",
    "\n",
    "First let's create artificial data that we will extract later batch by batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[132 119]\n",
      "  [126 119]]\n",
      "\n",
      " [[133 126]\n",
      "  [144 140]]\n",
      "\n",
      " [[126 129]\n",
      "  [116 146]]\n",
      "\n",
      " [[145 104]\n",
      "  [143 143]]\n",
      "\n",
      " [[114 122]\n",
      "  [102 148]]\n",
      "\n",
      " [[122 118]\n",
      "  [145 134]]\n",
      "\n",
      " [[131 134]\n",
      "  [122 104]]\n",
      "\n",
      " [[145 103]\n",
      "  [136 138]]\n",
      "\n",
      " [[128 119]\n",
      "  [141 118]]\n",
      "\n",
      " [[106 115]\n",
      "  [124 130]]]\n",
      "labels: [3 5 8 4 0 9 1 6 7 2]\n"
     ]
    }
   ],
   "source": [
    "data = np.random.randint(100,150, size = (10,2,2))\n",
    "labels = np.random.permutation(10)\n",
    "print(data)\n",
    "print(\"labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend that the above dataset is huge and we need to extract chunks of it. Now we will write a generator to extract from the above data a batch of two items, two data points and corresponding two labels. In deep learning applications, we want our data to be shuffled between epochs. For the first run, we can shuffle the data itself and from next epoch onwards generator will shuffle it for us. And the generator must run indefinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_gen(data, labels, batch_size = 2):\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i*batch_size >= len(labels):\n",
    "            i = 0\n",
    "            idx = np.random.permutation(len(labels))\n",
    "            data, labels = data[idx], labels[idx]\n",
    "            continue\n",
    "        else:\n",
    "            X = data[i*batch_size:(i+1)*batch_size,:]\n",
    "            y = labels[i*batch_size:(i+1)*batch_size]\n",
    "            i += 1\n",
    "            yield X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that we have conveniently glossed over a technical point here. As the data is a numpy ndarry, to extract parts of it, we have to first load it. If our data set is huge, this method fails there. But there are ways to work around this problem. First, we can read numpy files without loading the whole file into RAM. More details can be found [here](https://stackoverflow.com/questions/42727412/efficient-way-to-partially-read-large-numpy-file). Secondly, in deep learning we encounter multiple files each of small size. In that case we can create a dictionary of indexes and file names and then load only a few of those as per index value. These modifications can be easily incorporated as per our need. Details can be found [here](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly).\n",
    "\n",
    "Now that we have created a generator, we have to test it to see whether it functions as intended or not. So we will extract 10 batches of size 2 each from the (data, labels) pair and see. Here we have assumed that our original data is shuffled. If it is not, we can easily shuffle it by using \"np.shuffle()\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[132 119]\n",
      "  [126 119]]\n",
      "\n",
      " [[133 126]\n",
      "  [144 140]]] [3 5]\n",
      "(2, 2, 2) (2,)\n",
      "=========================\n",
      "[[[126 129]\n",
      "  [116 146]]\n",
      "\n",
      " [[145 104]\n",
      "  [143 143]]] [8 4]\n",
      "(2, 2, 2) (2,)\n",
      "=========================\n",
      "[[[114 122]\n",
      "  [102 148]]\n",
      "\n",
      " [[122 118]\n",
      "  [145 134]]] [0 9]\n",
      "(2, 2, 2) (2,)\n",
      "=========================\n",
      "[[[131 134]\n",
      "  [122 104]]\n",
      "\n",
      " [[145 103]\n",
      "  [136 138]]] [1 6]\n",
      "(2, 2, 2) (2,)\n",
      "=========================\n",
      "[[[128 119]\n",
      "  [141 118]]\n",
      "\n",
      " [[106 115]\n",
      "  [124 130]]] [7 2]\n",
      "(2, 2, 2) (2,)\n",
      "=========================\n",
      "[[[132 119]\n",
      "  [126 119]]\n",
      "\n",
      " [[145 104]\n",
      "  [143 143]]] [3 4]\n",
      "(2, 2, 2) (2,)\n",
      "=========================\n",
      "[[[131 134]\n",
      "  [122 104]]\n",
      "\n",
      " [[126 129]\n",
      "  [116 146]]] [1 8]\n",
      "(2, 2, 2) (2,)\n",
      "=========================\n",
      "[[[133 126]\n",
      "  [144 140]]\n",
      "\n",
      " [[106 115]\n",
      "  [124 130]]] [5 2]\n",
      "(2, 2, 2) (2,)\n",
      "=========================\n",
      "[[[114 122]\n",
      "  [102 148]]\n",
      "\n",
      " [[122 118]\n",
      "  [145 134]]] [0 9]\n",
      "(2, 2, 2) (2,)\n",
      "=========================\n",
      "[[[128 119]\n",
      "  [141 118]]\n",
      "\n",
      " [[145 103]\n",
      "  [136 138]]] [7 6]\n",
      "(2, 2, 2) (2,)\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "get_data = my_gen(data,labels)\n",
    "for i in range(10):\n",
    "    X,y = next(get_data)\n",
    "    print(X,y)\n",
    "    print(X.shape, y.shape)\n",
    "    print(\"=========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above generator code, we manually shuffled the data between epochs. But in keras we can use Sequence class to do this for us automatically. The added advantage of using this class is that we can use multiprocessing capabilities. So the new generator code becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow Version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_new_gen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, labels, batch_size= 2 ):\n",
    "        self.x, self.y = data, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(self.x.shape[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return tf.math.floor(self.x.shape[0] / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_x = self.x[inds]\n",
    "        batch_y = self.y[inds]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we must add `len` method and `getitem` method within the class and if we want to shuffle data between epochs, we have to add `on_epoch_end()` method. `len` finds out the number of batches possible in an epoch and `getitem` extracts batches one by one. When one epoch is complete, `on_epoch_end()` shuffles the data and the process continues. We will test it with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[132 119]\n",
      "  [126 119]]\n",
      "\n",
      " [[133 126]\n",
      "  [144 140]]] [3 5]\n",
      "(2, 2, 2) (2,)\n",
      "===========================\n",
      "[[[126 129]\n",
      "  [116 146]]\n",
      "\n",
      " [[145 104]\n",
      "  [143 143]]] [8 4]\n",
      "(2, 2, 2) (2,)\n",
      "===========================\n",
      "[[[114 122]\n",
      "  [102 148]]\n",
      "\n",
      " [[122 118]\n",
      "  [145 134]]] [0 9]\n",
      "(2, 2, 2) (2,)\n",
      "===========================\n",
      "[[[131 134]\n",
      "  [122 104]]\n",
      "\n",
      " [[145 103]\n",
      "  [136 138]]] [1 6]\n",
      "(2, 2, 2) (2,)\n",
      "===========================\n",
      "[[[128 119]\n",
      "  [141 118]]\n",
      "\n",
      " [[106 115]\n",
      "  [124 130]]] [7 2]\n",
      "(2, 2, 2) (2,)\n",
      "===========================\n",
      "[[[145 103]\n",
      "  [136 138]]\n",
      "\n",
      " [[133 126]\n",
      "  [144 140]]] [6 5]\n",
      "(2, 2, 2) (2,)\n",
      "===========================\n",
      "[[[126 129]\n",
      "  [116 146]]\n",
      "\n",
      " [[122 118]\n",
      "  [145 134]]] [8 9]\n",
      "(2, 2, 2) (2,)\n",
      "===========================\n",
      "[[[145 104]\n",
      "  [143 143]]\n",
      "\n",
      " [[128 119]\n",
      "  [141 118]]] [4 7]\n",
      "(2, 2, 2) (2,)\n",
      "===========================\n",
      "[[[131 134]\n",
      "  [122 104]]\n",
      "\n",
      " [[114 122]\n",
      "  [102 148]]] [1 0]\n",
      "(2, 2, 2) (2,)\n",
      "===========================\n",
      "[[[132 119]\n",
      "  [126 119]]\n",
      "\n",
      " [[106 115]\n",
      "  [124 130]]] [3 2]\n",
      "(2, 2, 2) (2,)\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "get_new_data = my_new_gen(data, labels)\n",
    "\n",
    "for i in range(10):\n",
    "    if i == 5:\n",
    "        get_new_data.on_epoch_end()\n",
    "        i = 0\n",
    "    elif i > 5:\n",
    "        i = i-5\n",
    "    dat,labs = get_new_data.__getitem__(i)\n",
    "    print(dat,labs)\n",
    "    print(dat.shape, labs.shape)\n",
    "    print(\"===========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the generators work fine. Now we will use it to implement a CNN model on MNIST data. Note that this example is bit stretched and strange. We don't need generators to implement small data sets like MNIST. Whole of MNIST can be loaded into RAM. By this example the aim is just to show a different way of implementing it using generators. Of course the codes can be modified to handle cases where we indeed need generators to do analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels),(test_data,test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.reshape(60000,28,28,1)/255.\n",
    "id = np.random.permutation(len(train_labels))\n",
    "training_data, training_labels = train_data[id[0:48000]], train_labels[id[0:48000]]\n",
    "val_data, val_labels = train_data[id[48000:60000]], train_labels[id[48000:60000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.Conv2D(32, 3, activation = 'relu', input_shape = (28,28,1)),\n",
    "    layers.MaxPool2D(2),\n",
    "    layers.Conv2D(64,5,activation = 'relu'),\n",
    "    layers.MaxPool2D(2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32,activation = 'relu'),\n",
    "    layers.Dense(10, activation = 'sigmoid')\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras requires the generator to run indefinitely\n",
    "class data_gen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, labels, batch_size=128):\n",
    "        self.x, self.y = data, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(self.x.shape[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(tf.math.ceil(self.x.shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_x = self.x[inds]\n",
    "        batch_y = self.y[inds]\n",
    "        return batch_x, tf.keras.utils.to_categorical(batch_y)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = data_gen(train_data, train_labels,batch_size = 128)\n",
    "val_gen = data_gen(val_data, val_labels,batch_size = 128)\n",
    "batch_size = 128\n",
    "steps_per_epoch = np.floor(len(train_labels)/batch_size)\n",
    "val_steps = np.floor(len(val_labels)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "468/468 [==============================] - 10s 10ms/step - loss: 0.5769 - accuracy: 0.8351 - val_loss: 0.0858 - val_accuracy: 0.9716\n",
      "Epoch 2/10\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 0.0795 - accuracy: 0.9756 - val_loss: 0.0454 - val_accuracy: 0.9860\n",
      "Epoch 3/10\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 0.0512 - accuracy: 0.9839 - val_loss: 0.0377 - val_accuracy: 0.9883\n",
      "Epoch 4/10\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 0.0389 - accuracy: 0.9879 - val_loss: 0.0278 - val_accuracy: 0.9908\n",
      "Epoch 5/10\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 0.0299 - accuracy: 0.9908 - val_loss: 0.0279 - val_accuracy: 0.9899\n",
      "Epoch 6/10\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 0.0238 - accuracy: 0.9922 - val_loss: 0.0170 - val_accuracy: 0.9950\n",
      "Epoch 7/10\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 0.0214 - accuracy: 0.9931 - val_loss: 0.0118 - val_accuracy: 0.9966\n",
      "Epoch 8/10\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 0.0158 - accuracy: 0.9950 - val_loss: 0.0146 - val_accuracy: 0.9952\n",
      "Epoch 9/10\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 0.0141 - accuracy: 0.9955 - val_loss: 0.0107 - val_accuracy: 0.9974\n",
      "Epoch 10/10\n",
      "468/468 [==============================] - 3s 7ms/step - loss: 0.0128 - accuracy: 0.9957 - val_loss: 0.0078 - val_accuracy: 0.9977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2543f4e64c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_gen, steps_per_epoch = steps_per_epoch, epochs = 10,\n",
    "          validation_data = val_gen, validation_steps = val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.0282 - accuracy: 0.9922\n",
      "Test Loss: 0.0281691811978817\n",
      "Test Accuracy: 0.9922000169754028\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data.reshape(10000,28,28,1)/255., tf.keras.utils.to_categorical(test_labels), verbose = 2)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reached close to 99% accuracy which is not bad! This example might seem a bit stretched as we don't need generators for small datasets like MNIST. The aim of the example is just to show different implementation using generators. \n",
    "\n",
    "Perhaps the most detailed blog about using generators for deep learning is [this one](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly). I also found [these comments](https://github.com/keras-team/keras/issues/9707#issuecomment-374609666) helpful.\n",
    "\n",
    "Update 1: With the release of `Tensorflow-2.0`, it is much easier to use `tf.data.Dataset` API for handling large datasets. Generators can still be used for training using `tf.keras`. As a final note, use generators if it is absolutely essential to do so. Otherwise, use `tf.data.Dataset` API. Check out [this post](https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2/) for an end-to-end data pipeline and training using generators in `Tensorflow 2`.\n",
    "\n",
    "Update 2: See [this blog](https://biswajitsahoo1111.github.io/post/reading-multiple-files-in-tensorflow-2-using-sequence/) for a complete workflow for reading multiple files using `Tensorflow Sequence`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_24_env",
   "language": "python",
   "name": "tf_24_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
