{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading multiple csv files in PyTorch\n",
    "Author: [Biswajit Sahoo](https://biswajitsahoo1111.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/biswajitsahoo1111/blog_notebooks/blob/master/Reading_multiple_csv_files_in_PyTorch.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/biswajitsahoo1111/blog_notebooks/blob/master/Reading_multiple_csv_files_in_PyTorch.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://www.dropbox.com/s/qxz8zmctpopulck/Reading_multiple_csv_files_in_PyTorch.ipynb?dl=1\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUKXGzb6-rxG"
   },
   "source": [
    "In many engineering applications data are usually stored in CSV (Comma Separated Values) files. In big data applications, it's not uncommon to obtain thousands of csv files. As the number of files increases, at some point, we can no longer load the whole dataset into computer's memory. In deep learning applications it is increasingly common to come across datasets that don't fit in the computer's memory. In that case, we have to devise a way so as to be able to read chunks of data at a time so that the model can be trained using the whole dataset.\n",
    "\n",
    "There are many ways to achieve this objective. In this post, we will adopt an approach that allows us to read csv files in chunks and preprocess those files in whatever way we like. Then we can pass the processed data to train any deep learning model. Though we will use csv files in this post, the method is general enough to work for other file formats (such as .txt, .npz, etc.) as well. We will demonstrate the procedure using 500 csv files. But the method can be easily extended to huge datasets involving thousands of csv files. \n",
    "\n",
    "This post is self-sufficient in the sense that readers don’t have to download any data from anywhere. Just run the following codes sequentially. First, a folder named “random_data” will be created in current working directory and .csv files will be saved in it. Subsequently, files will be read from that folder and processed. Just make sure that your current working directory doesn’t have an old folder named “random_data”. Then run the following code cells. We will use PyTorch to run our deep learning model. For efficiency in data loading, we will use PyTorch dataloaders.\n",
    "\n",
    "## Outline:\n",
    "1. [Create 500 \".csv\" files and save it in the folder “random_data” in current working directory.](#sec_1)\n",
    "2. [Create a custom dataloader.](#sec_2)\n",
    "3. [Feed the chunks of data to a CNN model and train it for several epochs.](#sec_3)\n",
    "4. [Make prediction on new data for which labels are not knownn.](#sec_4)\n",
    "\n",
    "<a id=\"sec_1\"></a>\n",
    "\n",
    "## 1. Create 500 .csv files of random data\n",
    "\n",
    "As we intend to train a CNN model for classification using our data, we will generate data for 5 different classes. The dataset that we will create is a contrived one. But readers can modify the approach slightly to cater to their need. Following is the process that we will follow.\n",
    "\n",
    "* Each .csv file will have one column of data with 1024 entries.\n",
    "* Each file will be saved using one of the following names (Fault_1, Fault_2, Fault_3, Fault_4, Fault_5). The dataset is balanced, meaning, for each category, we have approximately same number of observations. Data files in \"Fault_1\" category will have names as \"Fault_1_001.csv\", \"Fault_1_002.csv\", \"Fault_1_003.csv\", ..., \"Fault_1_100.csv\". Similarly for other classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xxGmYG9yaOo4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "np.random.seed(1111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y13lowGcZNxT"
   },
   "source": [
    "First create a function that will generate random files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sllGpYsxac2p"
   },
   "outputs": [],
   "source": [
    "def create_random_csv_files(fault_classes, number_of_files_in_each_class):\n",
    "    os.mkdir(\"./random_data/\")  # Make a directory to save created files.\n",
    "    for fault_class in fault_classes:\n",
    "        for i in range(number_of_files_in_each_class):\n",
    "            data = np.random.rand(1024,)\n",
    "            file_name = \"./random_data/\" + eval(\"fault_class\") + \"_\" + \"{0:03}\".format(i+1) + \".csv\" # This creates file_name\n",
    "            np.savetxt(eval(\"file_name\"), data, delimiter = \",\", header = \"V1\", comments = \"\")\n",
    "        print(str(eval(\"number_of_files_in_each_class\")) + \" \" + eval(\"fault_class\") + \" files\"  + \" created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGRifPjQZUgG"
   },
   "source": [
    "Now use the function to create 100 files each for five fault types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggzaCUJxakli",
    "outputId": "46429325-aec4-4d95-f10b-c00736df5a77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Fault_1 files created.\n",
      "100 Fault_2 files created.\n",
      "100 Fault_3 files created.\n",
      "100 Fault_4 files created.\n",
      "100 Fault_5 files created.\n"
     ]
    }
   ],
   "source": [
    "create_random_csv_files([\"Fault_1\", \"Fault_2\", \"Fault_3\", \"Fault_4\", \"Fault_5\"], number_of_files_in_each_class = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81gQNqEeaohQ",
    "outputId": "1692cac3-e51a-4019-fdd1-b860aa55c304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  500\n",
      "Showing first 10 files...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./random_data\\\\Fault_1_001.csv',\n",
       " './random_data\\\\Fault_1_002.csv',\n",
       " './random_data\\\\Fault_1_003.csv',\n",
       " './random_data\\\\Fault_1_004.csv',\n",
       " './random_data\\\\Fault_1_005.csv',\n",
       " './random_data\\\\Fault_1_006.csv',\n",
       " './random_data\\\\Fault_1_007.csv',\n",
       " './random_data\\\\Fault_1_008.csv',\n",
       " './random_data\\\\Fault_1_009.csv',\n",
       " './random_data\\\\Fault_1_010.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob(\"./random_data/*\")\n",
    "print(\"Total number of files: \", len(files))\n",
    "print(\"Showing first 10 files...\")\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJnDg7qbZcwb"
   },
   "source": [
    "To extract labels from file name, extract the part of the file name that corresponds to fault type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KCG3_9L5ZfhR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./random_data\\Fault_1_001.csv\n"
     ]
    }
   ],
   "source": [
    "print(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pIj3IRkvZiHj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fault_1\n"
     ]
    }
   ],
   "source": [
    "print(files[0][14:21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9-TnkHCZlbo"
   },
   "source": [
    "Now that data have been created, we will go to the next step. That is, create a custom dataloader, preprocess the time series like data into a matrix like shape such that a 2-D CNN can ingest it. We reshape the data in that way to just illustrate the point. Readers should use their own preprocessing steps.\n",
    "\n",
    "<a id=\"sec_2\"></a>\n",
    "\n",
    "## 2. Write a custom dataloader\n",
    "\n",
    "We have to first create a `Dataset` class. Then we can pass the dataset to the dataloader. Every dataset class must implement the `__len__` method that determines the length of the dataset and `__getitem__` method that iterates over the dataset item by item. In our case, item would mean the processed version of a chunk of data.\n",
    "\n",
    "The following dataset class takes a list of file names as first argument. The second argument is batch_size. batch_size determines how many files we will process at one go. As we will be solving a classification problem, we have to assign labels to each raw data. We will use following labels for convenience.\n",
    "\n",
    "|Class|Label|\n",
    "|:----:|:----:|\n",
    "|Fault_1|0|\n",
    "|Fault_2|1|\n",
    "|Fault_3|2|\n",
    "|Fault_4|3|\n",
    "|Fault_5|4|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "soLWXQUlarRa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.7.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re   \n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "print(\"PyTorch Version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yKO60WLHa0Vh"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, filenames, batch_size):\n",
    "    # `filenames` is a list of strings the contains all file names.\n",
    "    # `batch_size` is the determines the number of files that we want to read in a chunk.\n",
    "        self.filenames= filenames\n",
    "        self.batch_size = batch_size\n",
    "  def __len__(self):\n",
    "        return int(np.ceil(len(self.filenames) / float(self.batch_size)))   # Number of chunks.\n",
    "  def __getitem__(self, idx): #idx means index of the chunk.\n",
    "    # In this method, we do all the preprocessing.\n",
    "    # First read data from files in a chunk. Preprocess it. Extract labels. Then return data and labels.\n",
    "        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]   # This extracts one batch of file names from the list `filenames`.\n",
    "        data = []\n",
    "        labels = []\n",
    "        label_classes = [\"Fault_1\", \"Fault_2\", \"Fault_3\", \"Fault_4\", \"Fault_5\"]\n",
    "        for file in batch_x:\n",
    "            temp = pd.read_csv(open(file,'r')) # Change this line to read any other type of file\n",
    "            data.append(temp.values.reshape(32,32,1)) # Convert column data to matrix like data with one channel\n",
    "            pattern = \"^\" + eval(\"file[14:21]\")      # Pattern extracted from file_name\n",
    "            for j in range(len(label_classes)):\n",
    "                if re.match(pattern, label_classes[j]): # Pattern is matched against different label_classes\n",
    "                    labels.append(j)  \n",
    "        data = np.asarray(data).reshape(-1,1,32,32) # Because of Pytorch's channel first convention\n",
    "        labels = np.asarray(labels)\n",
    "\n",
    "        # The following condition is actually needed in Pytorch. Otherwise, for our particular example, the iterator will be an infinite loop.\n",
    "        # Readers can verify this by removing this condition.\n",
    "        if idx == self.__len__():  \n",
    "          raise IndexError\n",
    "\n",
    "        return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read any other file format, inside the `__getitem__` method change the line that reads files. This will enable us to read different file formats, be it `.txt` or `.npz` or any other. Preprocessing of data, different from what we have done in this blog, can be done within the `__getitem__` method.\n",
    "\n",
    "Now we will check whether the dataset works as intended or not. We will set batch_size to 10. This means that files in chunks of 10 will be read and processed. The list of files from which 10 are chosen can be an ordered file list or shuffled list. In case, the files are not shuffled, use np.random.shuffle(file_list) to shuffle files.\n",
    "\n",
    "In the demonstration, we will read files from an ordered list. This will help us check any errors in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hqsNzcA4bPmA"
   },
   "outputs": [],
   "source": [
    "check_dataset = CustomDataset(filenames = files, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BTpuWZH8OHoG",
    "outputId": "0a6bf398-4201-4b21-c3f2-937119ecce6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Epf0nPFYbYTR",
    "outputId": "b86dc58a-60f6-4e7f-ae6a-beb7891881cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 32, 32) (10,)\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "(10, 1, 32, 32) (10,)\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "(10, 1, 32, 32) (10,)\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "(10, 1, 32, 32) (10,)\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "(10, 1, 32, 32) (10,)\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "(10, 1, 32, 32) (10,)\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for i, (data, labels) in enumerate(check_dataset):\n",
    "  print(data.shape, labels.shape)\n",
    "  print(labels)\n",
    "  if i == 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above cell multiple times to observe different labels. Label 1 appears only when all the files corresponding to “Fault_1” have been read. There are 100 files for “Fault_1” and we have set batch_size to 10. In the above cell we are iterating over the generator only 6 times. When number of iterations become greater than 10, we see label 1 and subsequently other labels. This will happen only if our initial file list is not shuffled. If the original list is shuffled, we will get random labels.\n",
    "\n",
    "To train a deep learning model, we need to create a data loader from the dataset. Dataloaders offer multi-worker, multi-processing capabilities without requiring us to right codes for that. So let's first create a dataloader from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wixLWbJmcrYG"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Xxx3_a6tc1rE"
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(check_dataset,batch_size = None, shuffle = True) # Here we select batch size to be None as we have already batched our data in dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AIwCXEiwhoa"
   },
   "source": [
    "Check whether dataloader works on not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOfh48dfdh5g",
    "outputId": "92feb299-1887-45bf-c123-6f2f5dfd8b31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 32, 32]) torch.Size([10])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "torch.Size([10, 1, 32, 32]) torch.Size([10])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "torch.Size([10, 1, 32, 32]) torch.Size([10])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.int32)\n",
      "torch.Size([10, 1, 32, 32]) torch.Size([10])\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "for i, (data,labels) in enumerate(dataloader):\n",
    "    print(data.shape, labels.shape)\n",
    "    print(labels)  # Just to see the labels.\n",
    "    if i == 3: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYzSoBi2wn_3"
   },
   "source": [
    "Now that dataloader works, we will use it to train a simple deep learning model. The focus of this post is not on the model itself. So we will use a simplest model. If readers want a different model, they can do so by just replacing our model with theirs.\n",
    "\n",
    "<a id=\"sec_3\"></a>\n",
    "\n",
    "## 3. Feed chunks of data to a CNN model and train it for several epochs\n",
    "\n",
    "But before we build the model and train it, we will first move our files to different folders depending on their fault type. We do so as it will be convenient for later to create a training, validation, and test set from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wieMM7RHxxwm"
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MN1nE-gIxzNy"
   },
   "source": [
    "Create five different folders one each for a given fault type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wImmmV7sx7l1"
   },
   "outputs": [],
   "source": [
    "fault_folders = [\"Fault_1\", \"Fault_2\", \"Fault_3\", \"Fault_4\", \"Fault_5\"]\n",
    "for folder_name in fault_folders:\n",
    "    os.mkdir(os.path.join(\"./random_data\", folder_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsLYkdtWx-pa"
   },
   "source": [
    "Move files into those folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aOaqK1CuyFkJ"
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    pattern = \"^\" + eval(\"file[14:21]\")\n",
    "    for j in range(len(fault_folders)):\n",
    "        if re.match(pattern, fault_folders[j]):\n",
    "            dest = os.path.join(\"./random_data/\",eval(\"fault_folders[j]\"))\n",
    "            shutil.move(file, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kJB_6wpQyITN",
    "outputId": "e5ef0014-0941-4b10-9532-e544fa18ebbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./random_data\\\\Fault_1',\n",
       " './random_data\\\\Fault_2',\n",
       " './random_data\\\\Fault_3',\n",
       " './random_data\\\\Fault_4',\n",
       " './random_data\\\\Fault_5']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./random_data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h0SLtrHdyMmB",
    "outputId": "2c4772a4-d076-4cb9-b0fd-2cc02665874a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./random_data/Fault_1\\\\Fault_1_001.csv',\n",
       " './random_data/Fault_1\\\\Fault_1_002.csv',\n",
       " './random_data/Fault_1\\\\Fault_1_003.csv',\n",
       " './random_data/Fault_1\\\\Fault_1_004.csv',\n",
       " './random_data/Fault_1\\\\Fault_1_005.csv',\n",
       " './random_data/Fault_1\\\\Fault_1_006.csv',\n",
       " './random_data/Fault_1\\\\Fault_1_007.csv',\n",
       " './random_data/Fault_1\\\\Fault_1_008.csv',\n",
       " './random_data/Fault_1\\\\Fault_1_009.csv',\n",
       " './random_data/Fault_1\\\\Fault_1_010.csv']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./random_data/Fault_1/*\")[:10] # Showing first 10 files of Fault_1 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRfAbFpzyQaz",
    "outputId": "a4975318-a752-43a1-909a-f455ee3d7f86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./random_data/Fault_3\\\\Fault_3_001.csv',\n",
       " './random_data/Fault_3\\\\Fault_3_002.csv',\n",
       " './random_data/Fault_3\\\\Fault_3_003.csv',\n",
       " './random_data/Fault_3\\\\Fault_3_004.csv',\n",
       " './random_data/Fault_3\\\\Fault_3_005.csv',\n",
       " './random_data/Fault_3\\\\Fault_3_006.csv',\n",
       " './random_data/Fault_3\\\\Fault_3_007.csv',\n",
       " './random_data/Fault_3\\\\Fault_3_008.csv',\n",
       " './random_data/Fault_3\\\\Fault_3_009.csv',\n",
       " './random_data/Fault_3\\\\Fault_3_010.csv']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./random_data/Fault_3/*\")[:10] # Showing first 10 files of Fault_3 folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULDhruMvyUSl"
   },
   "source": [
    "Prepare the data for training set, validation set, and test_set. For each fault type, we will keep 70 files for training, 10 files for validation and 20 files for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1p4A5i-6ygoo"
   },
   "outputs": [],
   "source": [
    "fault_1_files = glob.glob(\"./random_data/Fault_1/*\")\n",
    "fault_2_files = glob.glob(\"./random_data/Fault_2/*\")\n",
    "fault_3_files = glob.glob(\"./random_data/Fault_3/*\")\n",
    "fault_4_files = glob.glob(\"./random_data/Fault_4/*\")\n",
    "fault_5_files = glob.glob(\"./random_data/Fault_5/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "vsdHq59uyj1n"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "WFn8u-YzynVP"
   },
   "outputs": [],
   "source": [
    "fault_1_train, fault_1_test = train_test_split(fault_1_files, test_size = 20, random_state = 5)\n",
    "fault_2_train, fault_2_test = train_test_split(fault_2_files, test_size = 20, random_state = 54)\n",
    "fault_3_train, fault_3_test = train_test_split(fault_3_files, test_size = 20, random_state = 543)\n",
    "fault_4_train, fault_4_test = train_test_split(fault_4_files, test_size = 20, random_state = 5432)\n",
    "fault_5_train, fault_5_test = train_test_split(fault_5_files, test_size = 20, random_state = 54321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "gVZILLw7yqyg"
   },
   "outputs": [],
   "source": [
    "fault_1_train, fault_1_val = train_test_split(fault_1_train, test_size = 10, random_state = 1)\n",
    "fault_2_train, fault_2_val = train_test_split(fault_2_train, test_size = 10, random_state = 12)\n",
    "fault_3_train, fault_3_val = train_test_split(fault_3_train, test_size = 10, random_state = 123)\n",
    "fault_4_train, fault_4_val = train_test_split(fault_4_train, test_size = 10, random_state = 1234)\n",
    "fault_5_train, fault_5_val = train_test_split(fault_5_train, test_size = 10, random_state = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "GllUgupHyuSb"
   },
   "outputs": [],
   "source": [
    "train_file_names = fault_1_train + fault_2_train + fault_3_train + fault_4_train + fault_5_train\n",
    "validation_file_names = fault_1_val + fault_2_val + fault_3_val + fault_4_val + fault_5_val\n",
    "test_file_names = fault_1_test + fault_2_test + fault_3_test + fault_4_test + fault_5_test\n",
    "\n",
    "# Shuffle training files (We don't need to shuffle validation and test data)\n",
    "np.random.shuffle(train_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MXxclRFiyy_I",
    "outputId": "4d27cc46-3bd8-48c5-8d74-d18e3e75fa3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train_files: 350\n",
      "Number of validation_files: 50\n",
      "Number of test_files: 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train_files:\" ,len(train_file_names))\n",
    "print(\"Number of validation_files:\" ,len(validation_file_names))\n",
    "print(\"Number of test_files:\" ,len(test_file_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrZ4FV3YzAHV"
   },
   "source": [
    "Create the datasets and dataloaders for training, validation, and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1wPJyfGmzHCl"
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_dataset = CustomDataset(filenames = train_file_names, batch_size = batch_size)\n",
    "val_dataset = CustomDataset(filenames = validation_file_names, batch_size = batch_size)\n",
    "test_dataset = CustomDataset(filenames = test_file_names, batch_size = batch_size)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = None, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = None)  # Shuffle is False by default.\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__h7jdZb0wY8"
   },
   "source": [
    "Now create the model. We will build one of the simplest models. Readers are free to choose a different model of their choice. If `torchsummary` is not installed, use `pip install torchsummary` to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "itihT7c_VH7Q"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Sequential, Conv2d, MaxPool2d, Flatten, Linear, ReLU, Softmax\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9lSXnCTbS5Hz",
    "outputId": "075c7cdf-46d5-44bd-ed85-d6edd5b85e2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 30, 30]             160\n",
      "              ReLU-2           [-1, 16, 30, 30]               0\n",
      "         MaxPool2d-3           [-1, 16, 15, 15]               0\n",
      "            Conv2d-4           [-1, 32, 13, 13]           4,640\n",
      "              ReLU-5           [-1, 32, 13, 13]               0\n",
      "         MaxPool2d-6             [-1, 32, 6, 6]               0\n",
      "           Flatten-7                 [-1, 1152]               0\n",
      "            Linear-8                   [-1, 16]          18,448\n",
      "              ReLU-9                   [-1, 16]               0\n",
      "           Linear-10                    [-1, 5]              85\n",
      "          Softmax-11                    [-1, 5]               0\n",
      "================================================================\n",
      "Total params: 23,333\n",
      "Trainable params: 23,333\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.35\n",
      "Params size (MB): 0.09\n",
      "Estimated Total Size (MB): 0.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(\n",
    "        Conv2d(in_channels = 1, out_channels = 16, kernel_size = 3),\n",
    "        ReLU(),\n",
    "        MaxPool2d(2),\n",
    "        Conv2d(16,32,3),\n",
    "        ReLU(),\n",
    "        MaxPool2d(2),\n",
    "        Flatten(),\n",
    "        Linear(in_features = 1152, out_features=16),\n",
    "        ReLU(),\n",
    "        Linear(16, 5),\n",
    "        Softmax(dim = 1)\n",
    ")\n",
    "model.to(dev)\n",
    "summary(model,input_size = (1,32,32), device = dev.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Xxr3z-IPcKlm"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBwhHpswcjvN",
    "outputId": "ff840748-170d-4490-bb64-492b737ef8da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Train_loss: 1.6115, Train_accuracy: 0.1829, Val_loss: 1.6098, Val_accuracy: 0.2000\n",
      "Epoch:1, Train_loss: 1.6099, Train_accuracy: 0.2000, Val_loss: 1.6098, Val_accuracy: 0.2000\n",
      "Epoch:2, Train_loss: 1.6098, Train_accuracy: 0.2000, Val_loss: 1.6098, Val_accuracy: 0.2000\n",
      "Epoch:3, Train_loss: 1.6098, Train_accuracy: 0.2000, Val_loss: 1.6098, Val_accuracy: 0.2000\n",
      "Epoch:4, Train_loss: 1.6097, Train_accuracy: 0.2000, Val_loss: 1.6098, Val_accuracy: 0.2000\n",
      "Epoch:5, Train_loss: 1.6096, Train_accuracy: 0.2000, Val_loss: 1.6098, Val_accuracy: 0.2000\n",
      "Epoch:6, Train_loss: 1.6096, Train_accuracy: 0.2000, Val_loss: 1.6098, Val_accuracy: 0.2000\n",
      "Epoch:7, Train_loss: 1.6094, Train_accuracy: 0.2000, Val_loss: 1.6098, Val_accuracy: 0.2000\n",
      "Epoch:8, Train_loss: 1.6092, Train_accuracy: 0.2000, Val_loss: 1.6098, Val_accuracy: 0.2000\n",
      "Epoch:9, Train_loss: 1.6088, Train_accuracy: 0.2000, Val_loss: 1.6098, Val_accuracy: 0.2000\n"
     ]
    }
   ],
   "source": [
    "model = model.float()   # We will make all model parameters floats.\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss_train = 0.0\n",
    "    running_loss_val = 0.0\n",
    "    correct_train = 0.0\n",
    "    correct_val = 0.0\n",
    "    num_labels_train = 0.0\n",
    "    num_labels_val = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(dev), labels.type(torch.LongTensor).to(dev) # PyTorch expects categorical targets as LongTensor.\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss_train = running_loss_train + loss.item()\n",
    "        correct_train = correct_train + (torch.argmax(outputs,dim = 1) == labels).float().sum()\n",
    "        num_labels_train = num_labels_train + len(labels)\n",
    "\n",
    "    # Validation loop\n",
    "    for inputs, labels in val_dataloader:\n",
    "        inputs, labels = inputs.to(dev), labels.type(torch.LongTensor).to(dev) # PyTorch expects categorical targets as LongTensor.\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss_val = running_loss_val + loss.item()\n",
    "        correct_val = correct_val + (torch.argmax(outputs, dim = 1) == labels).float().sum()\n",
    "        num_labels_val = num_labels_val + len(labels)\n",
    "\n",
    "    train_accuracy = correct_train/num_labels_train\n",
    "    val_accuracy = correct_val/num_labels_val\n",
    "    print(\"Epoch:{}, Train_loss: {:.4f}, Train_accuracy: {:.4f}, Val_loss: {:.4f}, Val_accuracy: {:.4f}\".\\\n",
    "        format(epoch, running_loss_train/len(train_dataloader), train_accuracy, running_loss_val/len(val_dataloader), val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWeS0eVt9H7O"
   },
   "source": [
    "Before we make any comments on training accuracy and validation accuracy, we should keep in mind that our original dataset contains only random numbers. So it would be better if we don't interpret the results here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4wgSZPL4i-G"
   },
   "source": [
    "Compute test score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJfKmwMS4qpL",
    "outputId": "290d8c7e-3627-4cc7-edf3-84b483586213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 1.6098, Test_accuracy: 0.2000\n"
     ]
    }
   ],
   "source": [
    "running_loss_test = 0.0\n",
    "correct_test = 0.0\n",
    "num_labels_test = 0.0\n",
    "for inputs, labels in test_dataloader:\n",
    "    inputs, labels = inputs.to(dev), labels.type(torch.LongTensor).to(dev) # Pytorch expects categorical targets as LongTensor.\n",
    "    outputs = model(inputs.float())\n",
    "    loss = criterion(outputs.to(dev), labels.to(dev))\n",
    "    running_loss_test = running_loss_test + loss.item()\n",
    "    correct_test = correct_test + (torch.argmax(outputs, dim = 1) == labels).float().sum()\n",
    "    num_labels_test = num_labels_test + len(labels)\n",
    "\n",
    "test_accuracy = correct_test/num_labels_test\n",
    "print(\"Test_loss: {:.4f}, Test_accuracy: {:.4f}\".format(running_loss_test/len(test_dataloader), test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYjy17y4pzgZ"
   },
   "source": [
    "<a id=\"sec_4\"></a>\n",
    "\n",
    "## 4. How to make predictions?\n",
    "\n",
    "Until now, we have evaluated our model on a kept out test set. For our test set, both data and labels were known. So we evaluated its performance. But oftentimes, for test set, we don’t have access to true labels. Rather, we have to make predictions on the data available. This is the case in online competitions where we have to submit our predictions on a test set for which we don’t know the labels. We will call this set (without any labels) the prediction set. This naming convention is arbitrary but we will stick with it.\n",
    "\n",
    "If the whole of our prediction set fits into memory, we can just make prediction on this data and then use `np.argmax()` or `torch.argmax()` to obtain predicted class labels. Otherwise, we can read files in prediction set in chunks, make predictions on the chunks and finally append our result.\n",
    "\n",
    "Yet another pedantic way of doing this is to write a separate dataset to read files from the prediction set in chunks and make predictions on it. We will show how this approach works. As we don’t have a prediction set yet, we will first create some files and save it to the prediction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "wpGUo4Csp4kJ"
   },
   "outputs": [],
   "source": [
    "def create_prediction_set(num_files = 20):\n",
    "    os.mkdir(\"./random_data/prediction_set\")\n",
    "    for i in range(num_files):\n",
    "        data = np.random.randn(1024,)\n",
    "        file_name = \"./random_data/prediction_set/\"  + \"file_\" + \"{0:03}\".format(i+1) + \".csv\" # This creates file_name\n",
    "        np.savetxt(eval(\"file_name\"), data, delimiter = \",\", header = \"V1\", comments = \"\")\n",
    "    print(str(eval(\"num_files\")) + \" \"+ \" files created in prediction set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some files for prediction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bvhdwzfq5NY",
    "outputId": "3a2b43b4-cf8c-4b56-eb04-f633fddc7753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55  files created in prediction set.\n"
     ]
    }
   ],
   "source": [
    "create_prediction_set(num_files = 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zCaGq8xIq9Td",
    "outputId": "97844b8d-3150-4566-acfe-247dd1f363fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  55\n",
      "Showing first 10 files...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./random_data/prediction_set\\\\file_001.csv',\n",
       " './random_data/prediction_set\\\\file_002.csv',\n",
       " './random_data/prediction_set\\\\file_003.csv',\n",
       " './random_data/prediction_set\\\\file_004.csv',\n",
       " './random_data/prediction_set\\\\file_005.csv',\n",
       " './random_data/prediction_set\\\\file_006.csv',\n",
       " './random_data/prediction_set\\\\file_007.csv',\n",
       " './random_data/prediction_set\\\\file_008.csv',\n",
       " './random_data/prediction_set\\\\file_009.csv',\n",
       " './random_data/prediction_set\\\\file_010.csv']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_files = glob.glob(\"./random_data/prediction_set/*\")\n",
    "print(\"Total number of files: \", len(prediction_files))\n",
    "print(\"Showing first 10 files...\")\n",
    "prediction_files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction dataset will be slightly different from our previous custom dataset class. We only need to return data in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "wujgThWTrEG_"
   },
   "outputs": [],
   "source": [
    "class PredictionDataset(Dataset):\n",
    "  def __init__(self, filenames, batch_size):\n",
    "        self.filenames= filenames\n",
    "        self.batch_size = batch_size\n",
    "  def __len__(self):\n",
    "        return int(np.ceil(len(self.filenames) / float(self.batch_size)))\n",
    "  def __getitem__(self, idx):\n",
    "        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        data = []\n",
    "        labels = []\n",
    "        label_classes = [\"Fault_1\", \"Fault_2\", \"Fault_3\", \"Fault_4\", \"Fault_5\"]\n",
    "        for file in batch_x:\n",
    "            temp = pd.read_csv(open(file,'r')) \n",
    "            data.append(temp.values.reshape(32,32,1)) \n",
    "        data = np.asarray(data).reshape(-1,1,32,32) \n",
    "        \n",
    "        \n",
    "        if idx == self.__len__():  \n",
    "          raise IndexError\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether the dataset and dataloader work or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "xsCndFnPrrt3"
   },
   "outputs": [],
   "source": [
    "prediction_dataset = PredictionDataset(prediction_files, batch_size = 10)\n",
    "prediction_dataloader = DataLoader(prediction_dataset,batch_size = None, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8SR1OKKYsAez",
    "outputId": "10eb62d3-a82d-407b-8308-9a438a8b0cd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 32, 32])\n",
      "torch.Size([10, 1, 32, 32])\n",
      "torch.Size([10, 1, 32, 32])\n",
      "torch.Size([10, 1, 32, 32])\n",
      "torch.Size([10, 1, 32, 32])\n",
      "torch.Size([5, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for data in prediction_dataloader:\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0U1VzypYsSMF",
    "outputId": "26d49df0-9917-4c66-c964-f689f2737cb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1909, 0.2369, 0.1795, 0.2210, 0.1718],\n",
       "        [0.1997, 0.2257, 0.1775, 0.2262, 0.1709],\n",
       "        [0.1945, 0.2329, 0.1789, 0.2226, 0.1711],\n",
       "        [0.1951, 0.2318, 0.1789, 0.2231, 0.1711],\n",
       "        [0.1923, 0.2352, 0.1793, 0.2217, 0.1716],\n",
       "        [0.1939, 0.2332, 0.1790, 0.2225, 0.1713],\n",
       "        [0.1959, 0.2307, 0.1787, 0.2236, 0.1711],\n",
       "        [0.1966, 0.2300, 0.1787, 0.2238, 0.1708],\n",
       "        [0.1992, 0.2268, 0.1781, 0.2253, 0.1706],\n",
       "        [0.1930, 0.2335, 0.1781, 0.2231, 0.1722],\n",
       "        [0.1947, 0.2323, 0.1790, 0.2229, 0.1712],\n",
       "        [0.1886, 0.2396, 0.1797, 0.2199, 0.1721],\n",
       "        [0.1879, 0.2406, 0.1798, 0.2195, 0.1722],\n",
       "        [0.2032, 0.2211, 0.1762, 0.2286, 0.1709],\n",
       "        [0.1843, 0.2443, 0.1792, 0.2187, 0.1735],\n",
       "        [0.2028, 0.2228, 0.1779, 0.2267, 0.1698],\n",
       "        [0.1916, 0.2360, 0.1794, 0.2214, 0.1717],\n",
       "        [0.2079, 0.2171, 0.1772, 0.2290, 0.1689],\n",
       "        [0.1945, 0.2325, 0.1790, 0.2228, 0.1712],\n",
       "        [0.1988, 0.2274, 0.1785, 0.2248, 0.1705],\n",
       "        [0.1962, 0.2325, 0.1775, 0.2239, 0.1700],\n",
       "        [0.1923, 0.2351, 0.1793, 0.2217, 0.1715],\n",
       "        [0.1959, 0.2308, 0.1788, 0.2235, 0.1710],\n",
       "        [0.1889, 0.2393, 0.1797, 0.2200, 0.1721],\n",
       "        [0.1939, 0.2332, 0.1791, 0.2225, 0.1713],\n",
       "        [0.1944, 0.2352, 0.1796, 0.2207, 0.1701],\n",
       "        [0.1922, 0.2353, 0.1793, 0.2216, 0.1716],\n",
       "        [0.1806, 0.2498, 0.1806, 0.2157, 0.1733],\n",
       "        [0.1990, 0.2272, 0.1784, 0.2249, 0.1704],\n",
       "        [0.1993, 0.2268, 0.1784, 0.2251, 0.1704],\n",
       "        [0.1901, 0.2379, 0.1795, 0.2206, 0.1719],\n",
       "        [0.1852, 0.2440, 0.1801, 0.2181, 0.1726],\n",
       "        [0.1963, 0.2286, 0.1763, 0.2262, 0.1726],\n",
       "        [0.1927, 0.2347, 0.1792, 0.2219, 0.1715],\n",
       "        [0.1961, 0.2306, 0.1788, 0.2235, 0.1709],\n",
       "        [0.1891, 0.2384, 0.1787, 0.2211, 0.1728],\n",
       "        [0.1982, 0.2282, 0.1785, 0.2245, 0.1706],\n",
       "        [0.1821, 0.2478, 0.1804, 0.2166, 0.1731],\n",
       "        [0.1957, 0.2311, 0.1789, 0.2233, 0.1710],\n",
       "        [0.1930, 0.2343, 0.1792, 0.2221, 0.1714],\n",
       "        [0.2005, 0.2255, 0.1782, 0.2256, 0.1702],\n",
       "        [0.2014, 0.2244, 0.1781, 0.2260, 0.1700],\n",
       "        [0.1854, 0.2437, 0.1801, 0.2183, 0.1726],\n",
       "        [0.1892, 0.2389, 0.1796, 0.2202, 0.1720],\n",
       "        [0.2009, 0.2250, 0.1782, 0.2258, 0.1701],\n",
       "        [0.1862, 0.2406, 0.1773, 0.2214, 0.1744],\n",
       "        [0.1954, 0.2314, 0.1789, 0.2232, 0.1710],\n",
       "        [0.1912, 0.2365, 0.1794, 0.2212, 0.1717],\n",
       "        [0.1935, 0.2319, 0.1767, 0.2248, 0.1731],\n",
       "        [0.1954, 0.2307, 0.1780, 0.2242, 0.1717],\n",
       "        [0.1928, 0.2346, 0.1792, 0.2219, 0.1715],\n",
       "        [0.1984, 0.2269, 0.1771, 0.2261, 0.1715],\n",
       "        [0.1959, 0.2308, 0.1788, 0.2235, 0.1710],\n",
       "        [0.1940, 0.2331, 0.1790, 0.2226, 0.1713],\n",
       "        [0.1956, 0.2312, 0.1789, 0.2233, 0.1710]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = []\n",
    "for data in prediction_dataloader:\n",
    "    data = data.to(dev)\n",
    "    preds.append(model(data.float()))\n",
    "preds = torch.cat(preds)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs of prediction are 5 dimensional vector. This is so because we have used 5 neurons in the output layer and our activation function is softmax. The 5 dimensional output vector for an input add to 1. So it can be interpreted as probability. Thus we should classify the input to a class, for which prediction probability is maximum. To get the class corresponding to maximum probability, we can use `np.argmax()` or `torch.argmax()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVVVzK04ts2Y",
    "outputId": "b8775877-8d46-47e8-9de2-de7279813dfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 3, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(preds, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our data are randomly generated. So we should not be surprised by this result.\n",
    "\n",
    "This brings us to the end of the blog. As we had planned in the beginning, we have created random data files, a custom dataloader, trained a model using that dataloader, and made predictions on new data. The above code can be tweaked slightly to read any type of files other than .csv. And now we can train our model without worrying about the data size. Whether the data size is 10GB or 750GB, our approach will work for both.\n",
    "\n",
    "Also note that we have not used the multi-worker and multi-processing capabilities of dataloader. To further speedup the dataloading process, readers should take advantage of the multiprocessing capabilities of dataloader. The best way to choose optimum multiprocessing and multi-worker parameters is to try a few ones and see which set of parameters work best for the system under consideration.\n",
    "\n",
    "As a final note, please keep in mind that the approach we have discussed in only one of many different ways in which we can read multiple files. I have chosen this approach as it seemed natural to me. I have neither strived for efficiency nor elegance. If readers have any better idea, I would be happy to know of it.\n",
    "\n",
    "I hope this blog would be of help to reader. Please bring any errors or omissions to my notice.\n",
    "\n",
    "**Disclaimer**: It is very likely that this blog might not have used some of the best practices of PyTorch. This is because the author has a superficial knowledge of PyTorch and is not aware of its best practices. The author (un)fortunately prefers `Tensorflow`. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Reading_multiple_csv_files_in_pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch17_env",
   "language": "python",
   "name": "torch17_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
